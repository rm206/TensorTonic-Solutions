import numpy as np

def adam_step(param, grad, m, v, t, lr=1e-3, beta1=0.9, beta2=0.999, eps=1e-8):
    """
    One Adam optimizer update step.
    Return (param_new, m_new, v_new).
    """
    param, grad, m, v = np.array(param), np.array(grad), np.array(m), np.array(v)
    m_new = (beta1 * m) + (1 - beta1) * grad
    v_new = (beta2 * v) + (1 - beta2) * grad**2
    m_correct = m_new / (1-beta1**t)
    v_correct = v_new / (1-beta2**t)
    param_new = param - lr * (m_correct / (v_correct**0.5 + eps))
    return (param_new, m_new, v_new)